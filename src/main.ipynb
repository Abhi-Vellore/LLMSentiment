{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Sentiment Analysis\n",
    "\n",
    "Project by Abhi Vellore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_process import AmazonDatasetPreprocessor, KaggleDatasetPreprocessor, SentenceDatasetPreprocessor\n",
    "from config import PROCESSED_DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "from gpt import ChatGPTSession\n",
    "import glob\n",
    "from llama import LLaMaSession\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from credentials import openai_api_key, llama_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_processor = AmazonDatasetPreprocessor('Amazon_Fashion_Review_Data.json')\n",
    "amazon_processor.preprocess()\n",
    "amazon_processor.to_csv('processed_amazon_data.csv')\n",
    "\n",
    "# Initialize and process the Sentence data\n",
    "sentence_processor = SentenceDatasetPreprocessor('Sentences_75Agree.txt')\n",
    "sentence_processor.preprocess()\n",
    "sentence_processor.to_csv('processed_sentence_data.csv')\n",
    "\n",
    "# Initialize and process the first Kaggle dataset\n",
    "kaggle1_processor = KaggleDatasetPreprocessor('kaggle_train.csv')\n",
    "kaggle1_processor.preprocess()\n",
    "\n",
    "# Initialize and process the second Kaggle dataset\n",
    "kaggle2_processor = KaggleDatasetPreprocessor('kaggle_test.csv')\n",
    "kaggle2_processor.preprocess()\n",
    "\n",
    "# Concatenate the preprocessed DataFrames\n",
    "processed_kaggle1_df = kaggle1_processor.df\n",
    "processed_kaggle2_df = kaggle2_processor.df\n",
    "combined_df = pd.concat([processed_kaggle1_df, processed_kaggle2_df], ignore_index=True)\n",
    "\n",
    "# Sample approximately 3000 rows from the combined DataFrame\n",
    "sampled_df = combined_df.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file in the processed directory\n",
    "combined_csv_path = os.path.join(PROCESSED_DATA_PATH, 'processed_kaggle_combined_data.csv')\n",
    "sampled_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"Processed combined Kaggle data saved to {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and Save Functions\n",
    "\n",
    "Create a series of functions with checks to prevent hitting OpenAI rate limit safeguards and track expenses of running the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define state-saving functions\n",
    "def save_state(state_file, last_processed_index):\n",
    "    with open(state_file, 'w') as file:\n",
    "        file.write(str(last_processed_index))\n",
    "\n",
    "def load_state(state_file):\n",
    "    try:\n",
    "        with open(state_file, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_limit = 10000\n",
    "rate_limit_per_minute = 500\n",
    "state_file_path = 'last_processed_line.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model\n",
    "\n",
    "Create GPT models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with GPT - saves into new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_GPTModel(input_csv_path, output_csv_path, chat_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = chat_session.send_prompt(row['Text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GPT Models\n",
    "\n",
    "Sets context to minimize tokens being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_context = \"As a sentiment analysis model, rate the sentiment of the following text from 1 to 5, where 1 is very negative and 5 is very positive. Provide only the number as a response.\"\n",
    "\n",
    "# Initialize sessions for GPT-3.5-Turbo and GPT-4.0\n",
    "session_gpt_3_5 = ChatGPTSession(api_key=openai_api_key, model='gpt-3.5-turbo', rate_limit_per_minute=1000)\n",
    "session_gpt_4 = ChatGPTSession(api_key=openai_api_key, model='gpt-4', rate_limit_per_minute=300)\n",
    "\n",
    "session_gpt_3_5.set_context(sentiment_context)\n",
    "session_gpt_4.set_context(sentiment_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Financial Data\n",
    "\n",
    "Split into two separate commands to ensure accuracy. Will use a for loop in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data_.csv' with GPT-3.5\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data.csv'\n",
    "output_sentence_path_3_5 = sentence_data_filename.replace('.csv', '_with_gpt_3.5.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_3_5, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_3_5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'sentence_data_with_gpt_3_5.csv' with GPT-4.0\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_3_5.csv'\n",
    "output_sentence_path_4 = sentence_data_filename.replace('_with_gpt_3_5.csv', '_with_gpt_scores.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing other datasets\n",
    "\n",
    "After confirming setup works for the financial dataset, process the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process other datasets with GPT-3.5 and GPT-4.0\n",
    "other_datasets = ['../data/processed/processed_amazon_data.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data.csv']\n",
    "\n",
    "for dataset in other_datasets:\n",
    "    # Process with GPT-3.5\n",
    "    output_path_3_5 = dataset.replace('.csv', '_with_gpt_3_5.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_3_5, session_gpt_3_5, \"GPT 3.5 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-3.5 and saved to {output_path_3_5}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-3.5 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run GPT-4.0 processing tomorrow.\")\n",
    "        continue  # Continue to the next dataset\n",
    "\n",
    "    # Process with GPT-4.0\n",
    "    output_path_4 = dataset.replace('.csv', '_with_gpt_scores.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-4.0 and saved to {output_path_4}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-4.0 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama Models\n",
    "Create Llama models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with Llama-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_llama_model(input_csv_path, output_csv_path, llama_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = llama_session.send_prompt(row['text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sessions for Llama-7b\n",
    "session_llama_7b = LLaMaSession(api_key=llama_api_key, model='llama-7b-chat', rate_limit_per_minute=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Financial Data\n",
    "\n",
    "Split into two separate commands to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data__with_gpt_scores.csv' with GPT-\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_scores.csv'\n",
    "output_sentence_path_7b = sentence_data_filename.replace('_with_gpt_scores.csv', '_all_scores.csv')\n",
    "process_dataset_with_llama_model(dataset, output_sentence_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with llama and saved to {output_sentence_path_7b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing other datasets\n",
    "\n",
    "After confirming setup works for the financial dataset, process the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_datasets = ['../data/processed/processed_amazon_data_with_gpt_scores.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data_with_gpt_scores.csv']\n",
    "\n",
    "# Process each dataset with both models\n",
    "for dataset in other_datasets:\n",
    "    output_path_7b = dataset.replace('.csv', '_all_scores.csv')\n",
    "    process_dataset_with_llama_model(dataset, output_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "    print(f\"Dataset processed with Llama-7B and saved to {output_path_7b}\")\n",
    "\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, model_name, dataset_name):\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    f1score = f1_score(actuals, predictions, average='weighted')\n",
    "    print(f\"Accuracy for {model_name} on {dataset_name}: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score for {model_name} on {dataset_name}: {f1score:.2f}\")\n",
    "    print(classification_report(actuals, predictions))\n",
    "    return accuracy, f1score\n",
    "\n",
    "def plot_confusion_matrix(predictions, actuals, model_name, dataset_name):\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name} - {dataset_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all datasets and plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['processed_sentence_data_all_scores.csv', 'processed_kaggle_combined_all_scores.csv', 'processed_amazon_data_all_scores.csv'] \n",
    "models = ['GPT_3_5', 'GPT_4', 'LLaMA_7B', 'LLaMA_13B']  # Replace with your actual model names\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    actuals = df['Actual']\n",
    "    for model in models:\n",
    "        predictions = df[model]\n",
    "        accuracy, f1score = evaluate_model(predictions, actuals, model, dataset_file)\n",
    "        plot_confusion_matrix(predictions, actuals, model, dataset_file)\n",
    "        metrics_df = metrics_df.append({\n",
    "            'Dataset': dataset_file,\n",
    "            'Model': model,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1score\n",
    "        }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all datasets together for aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "aggregated_metrics = metrics_df.groupby('Model').agg({'Accuracy': 'mean', 'F1_Score': 'mean'}).reset_index()\n",
    "\n",
    "# Plotting aggregated accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=aggregated_metrics)\n",
    "plt.title('Aggregated Accuracy for Each Model')\n",
    "plt.show()\n",
    "\n",
    "# Plotting aggregated F1-score for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='F1_Score', data=aggregated_metrics)\n",
    "plt.title('Aggregated F1 Score for Each Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets into a single DataFrame for correlation analysis\n",
    "combined_df = pd.DataFrame()\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    combined_df = combined_df.append(df, ignore_index=True)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = combined_df[models].corr()  # Use model names as column names\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Model Predictions')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
