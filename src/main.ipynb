{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhivellore/LLMSentiment/saenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pre_process import AmazonDatasetPreprocessor, KaggleDatasetPreprocessor, SentenceDatasetPreprocessor\n",
    "from config import PROCESSED_DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "from gpt import ChatGPTSession\n",
    "import glob\n",
    "from llama import LLaMaSession\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from credentials import openai_api_key, llama_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/processed/processed_amazon_data.csv\n",
      "Processed data saved to ../data/processed/processed_sentence_data.csv\n",
      "Processed combined Kaggle data saved to ../data/processed/processed_kaggle_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "amazon_processor = AmazonDatasetPreprocessor('Amazon_Fashion_Review_Data.json')\n",
    "amazon_processor.preprocess()\n",
    "amazon_processor.to_csv('processed_amazon_data.csv')\n",
    "\n",
    "# Initialize and process the Sentence data\n",
    "sentence_processor = SentenceDatasetPreprocessor('Sentences_75Agree.txt')\n",
    "sentence_processor.preprocess()\n",
    "sentence_processor.to_csv('processed_sentence_data.csv')\n",
    "\n",
    "# Initialize and process the first Kaggle dataset\n",
    "kaggle1_processor = KaggleDatasetPreprocessor('kaggle_train.csv')\n",
    "kaggle1_processor.preprocess()\n",
    "\n",
    "# Initialize and process the second Kaggle dataset\n",
    "kaggle2_processor = KaggleDatasetPreprocessor('kaggle_test.csv')\n",
    "kaggle2_processor.preprocess()\n",
    "\n",
    "# Concatenate the preprocessed DataFrames\n",
    "processed_kaggle1_df = kaggle1_processor.df\n",
    "processed_kaggle2_df = kaggle2_processor.df\n",
    "combined_df = pd.concat([processed_kaggle1_df, processed_kaggle2_df], ignore_index=True)\n",
    "\n",
    "# Sample approximately 3000 rows from the combined DataFrame\n",
    "sampled_df = combined_df.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file in the processed directory\n",
    "combined_csv_path = os.path.join(PROCESSED_DATA_PATH, 'processed_kaggle_combined_data.csv')\n",
    "sampled_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"Processed combined Kaggle data saved to {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define state-saving functions\n",
    "def save_state(state_file, last_processed_index):\n",
    "    with open(state_file, 'w') as file:\n",
    "        file.write(str(last_processed_index))\n",
    "\n",
    "def load_state(state_file):\n",
    "    try:\n",
    "        with open(state_file, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_limit = 10000\n",
    "rate_limit_per_minute = 500\n",
    "state_file_path = 'last_processed_line.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_GPTModel(input_csv_path, output_csv_path, chat_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = chat_session.send_prompt(row['text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_context = \"As a sentiment analysis model, rate the sentiment of the following text from 1 to 5, where 1 is very negative and 5 is very positive. Provide only the number as a response.\"\n",
    "\n",
    "# Initialize sessions for GPT-3.5-Turbo and GPT-4.0\n",
    "session_gpt_3_5 = ChatGPTSession(api_key=openai_api_key, model='gpt-3.5-turbo', rate_limit_per_minute=1000)\n",
    "session_gpt_4 = ChatGPTSession(api_key=openai_api_key, model='gpt-4.0', rate_limit_per_minute=300)\n",
    "\n",
    "session_gpt_3_5.set_context(sentiment_context)\n",
    "session_gpt_4.set_context(sentiment_context)\n",
    "\n",
    "# Process each dataset\n",
    "datasets_path = '../data/processed/*.csv'\n",
    "datasets = glob.glob(datasets_path)\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    output_path_3_5 = dataset.replace('.csv', '_with_gpt_3_5.csv')\n",
    "    process_dataset_with_GPTModel(dataset, output_path_3_5, session_gpt_3_5, \"GPT 3.5 Score\", state_file=state_file_path)\n",
    "    \n",
    "    # If the script has hit the daily limit, it will stop and needs to be run again the next day.\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break  # Stop processing files if the daily limit has been reached\n",
    "\n",
    "    output_path_4 = dataset.replace('.csv', '_with_gpt_4.csv')\n",
    "    process_dataset_with_GPTModel(dataset, output_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file=state_file_path)\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break  # Stop processing files if the daily limit has been reached\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_llama_model(input_csv_path, output_csv_path, llama_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = llama_session.send_prompt(row['text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sessions for GPT-3.5-Turbo and GPT-4.0\n",
    "session_llama_7b = LLaMaSession(api_key=llama_api_key, model='llama-7b-chat', rate_limit_per_minute=1000)\n",
    "session_llama_13b = LLaMaSession(api_key=llama_api_key, model='llama-13b-chat', rate_limit_per_minute=300)\n",
    "\n",
    "datasets_path = '../data/processed/*.csv'\n",
    "datasets = glob.glob(datasets_path)\n",
    "\n",
    "# Process each dataset with both models\n",
    "for dataset in datasets:\n",
    "    output_path_7b = dataset.replace('.csv', '_with_7b.csv')\n",
    "    process_dataset_with_llama_model(dataset, output_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break  # Stop processing files if the daily limit has been reached\n",
    "\n",
    "    \n",
    "    output_path_all = dataset.replace('.csv', '_with_all.csv')\n",
    "    process_dataset_with_llama_model(dataset, output_path_all, session_llama_13b, \"Llama 13B Score\", state_file=state_file_path)\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break  # Stop processing files if the daily limit has been reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, model_name, dataset_name):\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    f1score = f1_score(actuals, predictions, average='weighted')\n",
    "    print(f\"Accuracy for {model_name} on {dataset_name}: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score for {model_name} on {dataset_name}: {f1score:.2f}\")\n",
    "    print(classification_report(actuals, predictions))\n",
    "    return accuracy, f1score\n",
    "\n",
    "def plot_confusion_matrix(predictions, actuals, model_name, dataset_name):\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name} - {dataset_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all datasets and plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']  # Replace with your actual datasets\n",
    "models = ['GPT_3_5', 'GPT_4', 'LLaMA_7B', 'LLaMA_13B']  # Replace with your actual model names\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    actuals = df['Actual']\n",
    "    for model in models:\n",
    "        predictions = df[model]\n",
    "        accuracy, f1score = evaluate_model(predictions, actuals, model, dataset_file)\n",
    "        plot_confusion_matrix(predictions, actuals, model, dataset_file)\n",
    "        metrics_df = metrics_df.append({\n",
    "            'Dataset': dataset_file,\n",
    "            'Model': model,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1_Score': f1score\n",
    "        }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all datasets together for aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "aggregated_metrics = metrics_df.groupby('Model').agg({'Accuracy': 'mean', 'F1_Score': 'mean'}).reset_index()\n",
    "\n",
    "# Plotting aggregated accuracy for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=aggregated_metrics)\n",
    "plt.title('Aggregated Accuracy for Each Model')\n",
    "plt.show()\n",
    "\n",
    "# Plotting aggregated F1-score for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='F1_Score', data=aggregated_metrics)\n",
    "plt.title('Aggregated F1 Score for Each Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine all datasets into a single DataFrame for correlation analysis\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_file \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m      4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all datasets into a single DataFrame for correlation analysis\n",
    "combined_df = pd.DataFrame()\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    combined_df = combined_df.append(df, ignore_index=True)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = combined_df[models].corr()  # Use model names as column names\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Model Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = llama_api_key \n",
    "model = \"llama-7b-chat\"  \n",
    "rate_limit_per_minute = 120 \n",
    "\n",
    "# Initialize the LLaMa API session\n",
    "llama_session = LLaMaSession(api_key, model, rate_limit_per_minute)\n",
    "\n",
    "df = pd.read_csv('test_experiment.csv')\n",
    "\n",
    "# Define a function that will apply sentiment analysis to a text\n",
    "def apply_sentiment_analysis(text):\n",
    "    try:\n",
    "        return llama_session.analyze_sentiment(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing text '{text}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply sentiment analysis to each row of the DataFrame\n",
    "df['llama score'] = df['Text'].apply(apply_sentiment_analysis)\n",
    "\n",
    "# Save the DataFrame with the new \"llama score\" column to a new CSV file\n",
    "df.to_csv('test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Method to process all GPT\n",
    "def process_dataset_with_GPTmodel(input_csv_path, output_csv_path, chat_session, column_name):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"The dataset must have a 'text' column.\")\n",
    "    \n",
    "    # Apply the model to the \"text\" column and save the results in a new column\n",
    "    df[column_name] = df['text'].apply(lambda x: chat_session.send_prompt(x))\n",
    "    \n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
