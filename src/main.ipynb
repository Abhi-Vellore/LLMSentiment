{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Sentiment Analysis\n",
    "\n",
    "Project by Abhi Vellore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_process import AmazonDatasetPreprocessor, KaggleDatasetPreprocessor, SentenceDatasetPreprocessor\n",
    "from config import PROCESSED_DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "from gpt import ChatGPTSession\n",
    "import glob\n",
    "from llama import LLaMaSession\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from credentials import openai_api_key, llama_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/processed/processed_amazon_data.csv\n",
      "Processed data saved to ../data/processed/processed_sentence_data.csv\n",
      "Processed combined Kaggle data saved to ../data/processed/processed_kaggle_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "amazon_processor = AmazonDatasetPreprocessor('Amazon_Fashion_Review_Data.json')\n",
    "amazon_processor.preprocess()\n",
    "amazon_processor.to_csv('processed_amazon_data.csv')\n",
    "\n",
    "# Initialize and process the Sentence data\n",
    "sentence_processor = SentenceDatasetPreprocessor('Sentences_75Agree.txt')\n",
    "sentence_processor.preprocess()\n",
    "sentence_processor.to_csv('processed_sentence_data.csv')\n",
    "\n",
    "# Initialize and process the first Kaggle dataset\n",
    "kaggle1_processor = KaggleDatasetPreprocessor('kaggle_train.csv')\n",
    "kaggle1_processor.preprocess()\n",
    "\n",
    "# Initialize and process the second Kaggle dataset\n",
    "kaggle2_processor = KaggleDatasetPreprocessor('kaggle_test.csv')\n",
    "kaggle2_processor.preprocess()\n",
    "\n",
    "# Concatenate the preprocessed DataFrames\n",
    "processed_kaggle1_df = kaggle1_processor.df\n",
    "processed_kaggle2_df = kaggle2_processor.df\n",
    "combined_df = pd.concat([processed_kaggle1_df, processed_kaggle2_df], ignore_index=True)\n",
    "\n",
    "# Sample approximately 3000 rows from the combined DataFrame\n",
    "sampled_df = combined_df.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file in the processed directory\n",
    "combined_csv_path = os.path.join(PROCESSED_DATA_PATH, 'processed_kaggle_combined_data.csv')\n",
    "sampled_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"Processed combined Kaggle data saved to {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and Save Functions\n",
    "\n",
    "Create a series of functions with checks to prevent hitting OpenAI rate limit safeguards and track expenses of running the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define state-saving functions\n",
    "def save_state(state_file, last_processed_index):\n",
    "    with open(state_file, 'w') as file:\n",
    "        file.write(str(last_processed_index))\n",
    "\n",
    "def load_state(state_file):\n",
    "    try:\n",
    "        with open(state_file, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_limit = 10000\n",
    "rate_limit_per_minute = 500\n",
    "state_file_path = 'last_processed_line.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model\n",
    "\n",
    "Create GPT models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with GPT - saves into new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_GPTModel(input_csv_path, output_csv_path, chat_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = chat_session.send_prompt(row['Text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GPT Models\n",
    "\n",
    "Sets context to minimize tokens being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_context = \"As a sentiment analysis model, rate the sentiment of the following text from 1 to 5, where 1 is very negative and 5 is very positive. Provide only the number as a response.\"\n",
    "\n",
    "# Initialize sessions for GPT-3.5-Turbo and GPT-4.0\n",
    "session_gpt_3_5 = ChatGPTSession(api_key=openai_api_key, model='gpt-3.5-turbo', rate_limit_per_minute=1000)\n",
    "session_gpt_4 = ChatGPTSession(api_key=openai_api_key, model='gpt-4', rate_limit_per_minute=300)\n",
    "\n",
    "session_gpt_3_5.set_context(sentiment_context)\n",
    "session_gpt_4.set_context(sentiment_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing GPT 3.5\n",
    "\n",
    "Split into two separate commands to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data_.csv' with GPT-3.5\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data.csv'\n",
    "output_sentence_path_3_5 = sentence_data_filename.replace('.csv', '_with_gpt_3.5.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_3_5, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_3_5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j1dl302s3676d0rvtgvmmf5h0000gn/T/ipykernel_71396/4086550169.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Processing completed. Data saved to ../data/processed/processed_amazon_data_with_gpt_3_5.csv\n",
      "Dataset processed with GPT-3.5 and saved to ../data/processed/processed_amazon_data_with_gpt_3_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j1dl302s3676d0rvtgvmmf5h0000gn/T/ipykernel_71396/4086550169.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: I'm sorry, but I cannot provide a sentiment\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: The text is: \"I am feeling a bit\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Processing completed. Data saved to ../data/processed/processed_kaggle_combined_data_with_gpt_3_5.csv\n",
      "Dataset processed with GPT-3.5 and saved to ../data/processed/processed_kaggle_combined_data_with_gpt_3_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Process other datasets with GPT-3.5\n",
    "other_datasets = ['../data/processed/processed_amazon_data.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data.csv']\n",
    "\n",
    "for dataset in other_datasets:\n",
    "    save_state(state_file_path, -1)\n",
    "    # Process with GPT-3.5\n",
    "    output_path_3_5 = dataset.replace('.csv', '_with_gpt_3_5.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_3_5, session_gpt_3_5, \"GPT 3.5 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-3.5 and saved to {output_path_3_5}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-3.5 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run GPT-4.0 processing tomorrow.\")\n",
    "        continue  # Continue to the next dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j1dl302s3676d0rvtgvmmf5h0000gn/T/ipykernel_88503/4086550169.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Processing completed. Data saved to ../data/processed/processed_sentence_data_with_gpt_scores.csv\n",
      "Processed ../data/processed/processed_sentence_data_with_gpt_3_5.csv with GPT-4.0 and saved to ../data/processed/processed_sentence_data_with_gpt_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# Process 'sentence_data_with_gpt_3_5.csv' with GPT-4.0\n",
    "save_state(state_file_path, -1)\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_3_5.csv'\n",
    "output_sentence_path_4 = sentence_data_filename.replace('_with_gpt_3_5.csv', '_with_gpt_scores.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j1dl302s3676d0rvtgvmmf5h0000gn/T/ipykernel_88503/4086550169.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Sorry, but I can't provide a sentiment analysis\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: You didn't provide any text for me to analyze\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Processing completed. Data saved to ../data/processed/processed_amazon_data_with_gpt_3_5_with_gpt_scores.csv\n",
      "Dataset processed with GPT-4.0 and saved to ../data/processed/processed_amazon_data_with_gpt_3_5_with_gpt_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/j1dl302s3676d0rvtgvmmf5h0000gn/T/ipykernel_88503/4086550169.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: I'm sorry, but as an AI, I\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: You didn't provide any text for me to analyze\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: You didn't provide any text for me to analyze\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 1.3\n",
      "2.4\n",
      "3.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: I'm sorry, but I can't provide a\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: You didn't provide any text for me to analyze\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Sorry, there is no text provided for sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Could not convert response to int: Sorry, but I can't provide a sentiment analysis\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: You didn't provide any text for me to analyze\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: 3.5\n",
      "Could not convert response to int: 4.5\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Could not convert response to int: Please provide the text for sentiment analysis.\n",
      "Rate limit reached, sleeping for 60 seconds...\n",
      "Processing completed. Data saved to ../data/processed/processed_kaggle_combined_data_with_gpt_3_5_with_gpt_scores.csv\n",
      "Dataset processed with GPT-4.0 and saved to ../data/processed/processed_kaggle_combined_data_with_gpt_3_5_with_gpt_scores.csv\n"
     ]
    }
   ],
   "source": [
    "other_datasets = ['../data/processed/processed_amazon_data_with_gpt_3_5.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data_with_gpt_3_5.csv']  \n",
    "\n",
    "for dataset in other_datasets:\n",
    "    # Process with GPT-4.0\n",
    "    save_state(state_file_path, -1)\n",
    "    output_path_4 = dataset.replace('.csv', '_with_gpt_scores.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-4.0 and saved to {output_path_4}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-4.0 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        continue  # Continue to the next dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama Models\n",
    "Create Llama models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with Llama-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_llama_model(input_csv_path, output_csv_path, llama_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = llama_session.analyze_sentiment(row['Text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sessions for Llama-7b\n",
    "session_llama_7b = LLaMaSession(api_key=llama_api_key, model='llama-7b-chat', rate_limit_per_minute=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Sentence Data\n",
    "\n",
    "Split into two separate commands to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data__with_gpt_scores.csv' with GPT-\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_scores.csv'\n",
    "output_sentence_path_7b = sentence_data_filename.replace('_with_gpt_scores.csv', '_all_scores.csv')\n",
    "save_state(state_file_path, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No numeric response found in API response: ðŸ“ˆ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_dataset_with_llama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_data_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_sentence_path_7b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_llama_7b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLlama 7B Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence_data_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with llama and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_sentence_path_7b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m, in \u001b[0;36mprocess_dataset_with_llama_model\u001b[0;34m(input_csv_path, output_csv_path, llama_session, column_name, state_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Insert your API call here and store the response\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllama_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, column_name] \u001b[38;5;241m=\u001b[39m response\n\u001b[1;32m     29\u001b[0m  \u001b[38;5;66;03m# If starting from the beginning, write header\u001b[39;00m\n",
      "File \u001b[0;32m~/LLMSentiment/src/llama.py:23\u001b[0m, in \u001b[0;36mLLaMaSession.analyze_sentiment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     20\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(time_to_wait)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAs a sentiment analysis model, rate the sentiment of the following text from 1 to 5, where 1 is very negative and 5 is very positive. Provide only the number as a response.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Access the relevant part of the response\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/openai/_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/openai/_base_client.py:951\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    957\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/LLMSentiment/saenv/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_dataset_with_llama_model(sentence_data_filename, output_sentence_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with llama and saved to {output_sentence_path_7b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing other datasets\n",
    "\n",
    "After confirming setup works for the financial dataset, process the other dataset. Done separately because of time required for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_datasets = ['../data/processed/processed_amazon_data_with_gpt_scores.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data_with_gpt_scores.csv']\n",
    "\n",
    "# Process each dataset with both models\n",
    "for dataset in other_datasets:\n",
    "    save_state(state_file_path, -1)\n",
    "    output_path_7b = dataset.replace('.csv', '_all_scores.csv')\n",
    "    process_dataset_with_llama_model(dataset, output_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "    print(f\"Dataset processed with Llama-7B and saved to {output_path_7b}\")\n",
    "\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, model_name, dataset_name):\n",
    "    # Numeric evaluation\n",
    "    accuracy_numeric = accuracy_score(actuals, predictions)\n",
    "    f1score_numeric = f1_score(actuals, predictions, average='weighted')\n",
    "    report_numeric = classification_report(actuals, predictions)\n",
    "    \n",
    "    # Categorical mapping and evaluation\n",
    "    label_mapping = {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}\n",
    "    categorical_actuals = actuals.map(label_mapping)\n",
    "    categorical_predictions = predictions.map(label_mapping)\n",
    "    accuracy_categorical = accuracy_score(categorical_actuals, categorical_predictions)\n",
    "    f1score_categorical = f1_score(categorical_actuals, categorical_predictions, average='weighted', labels=['negative', 'neutral', 'positive'])\n",
    "    report_categorical = classification_report(categorical_actuals, categorical_predictions, labels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    # Store results in dictionaries\n",
    "    numeric_results = {\n",
    "        'Accuracy (Numeric)': accuracy_numeric,\n",
    "        'F1 Score (Numeric)': f1score_numeric,\n",
    "        'Classification Report (Numeric)': report_numeric\n",
    "    }\n",
    "    \n",
    "    categorical_results = {\n",
    "        'Accuracy (Categorical)': accuracy_categorical,\n",
    "        'F1 Score (Categorical)': f1score_categorical,\n",
    "        'Classification Report (Categorical)': report_categorical\n",
    "    }\n",
    "    \n",
    "    # Optionally, print the results\n",
    "    print(f\"--- Numeric Evaluation for {model_name} on {dataset_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_numeric:.2f}\")\n",
    "    print(f\"F1 Score: {f1score_numeric:.2f}\")\n",
    "    print(report_numeric)\n",
    "    \n",
    "    print(f\"--- Categorical Evaluation for {model_name} on {dataset_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_categorical:.2f}\")\n",
    "    print(f\"F1 Score: {f1score_categorical:.2f}\")\n",
    "    print(report_categorical)\n",
    "    \n",
    "    return numeric_results, categorical_results\n",
    "\n",
    "def plot_confusion_matrix(predictions, actuals, model_name, dataset_name):\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Define the labels for your classes\n",
    "    labels = [1, 2, 3, 4, 5]\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name} - {dataset_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics per Model per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['processed_sentence_data_all_scores.csv', 'processed_kaggle_combined_all_scores.csv', 'processed_amazon_data_all_scores.csv'] \n",
    "models = ['GPT_3_5', 'GPT_4', 'LLaMA_7B']  # Replace with your actual model names\n",
    "metrics_df = pd.DataFrame()\n",
    "all_scores_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    actuals = df['Actual_Score']\n",
    "    for model in models:\n",
    "        predictions = df[model]\n",
    "        numeric_results, categorical_results = evaluate_model(predictions, actuals, model, dataset_file)\n",
    "        plot_confusion_matrix(predictions, actuals, model, dataset_file)\n",
    "\n",
    "        # Flatten the results into a single dictionary for the DataFrame\n",
    "        results_to_store = {\n",
    "            'Dataset': dataset_file,\n",
    "            'Model': model,\n",
    "            'Accuracy (Numeric)': numeric_results['Accuracy (Numeric)'],\n",
    "            'F1 Score (Numeric)': numeric_results['F1 Score (Numeric)'],\n",
    "            'Accuracy (Categorical)': categorical_results['Accuracy (Categorical)'],\n",
    "            'F1 Score (Categorical)': categorical_results['F1 Score (Categorical)']\n",
    "        }\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([results_to_store])], ignore_index=True)\n",
    "\n",
    "        if all_scores_df.empty:\n",
    "            # If all_scores_df is empty, initialize it with the score columns\n",
    "            all_scores_df = df[models].copy()\n",
    "        else:\n",
    "            # If all_scores_df already contains data, concatenate along the columns (axis=1)\n",
    "            all_scores_df = pd.concat([all_scores_df, df[models]], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated Metrics Across all Datasets Per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results by taking the mean of each metric per model\n",
    "aggregated_metrics = metrics_df.groupby('Model').agg({\n",
    "    'Accuracy (Numeric)': 'mean',\n",
    "    'F1 Score (Numeric)': 'mean',\n",
    "    'Accuracy (Categorical)': 'mean',\n",
    "    'F1 Score (Categorical)': 'mean'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.35\n",
    "index = pd.Index(range(len(aggregated_metrics)))\n",
    "\n",
    "# Create combined plots for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index - bar_width/2, aggregated_metrics['Accuracy (Numeric)'], bar_width, label='Accuracy (Numeric)', color='lightgreen')\n",
    "plt.bar(index + bar_width/2, aggregated_metrics['Accuracy (Categorical)'], bar_width, label='Accuracy (Categorical)', color='lightblue')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(index, aggregated_metrics['Model'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparative Accuracy Across Models')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create combined plots for F1 score\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index - bar_width/2, aggregated_metrics['F1 Score (Numeric)'], bar_width, label='F1 Score (Numeric)', color='lightcoral')\n",
    "plt.bar(index + bar_width/2, aggregated_metrics['F1 Score (Categorical)'], bar_width, label='F1 Score (Categorical)', color='lightsalmon')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(index, aggregated_metrics['Model'])\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Comparative F1 Score Across Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions and actuals for each model\n",
    "model_predictions = {model: [] for model in models}\n",
    "actual_labels = []\n",
    "\n",
    "# Loop over the dataset files\n",
    "for dataset_file in datasets:\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    # Extend the actual labels list\n",
    "    actual_labels.extend(df['Actual_Score'])\n",
    "    \n",
    "    # For each model, extend the corresponding predictions list\n",
    "    for model in models:\n",
    "        model_predictions[model].extend(df[model])\n",
    "\n",
    "# Now create and plot a confusion matrix for each model\n",
    "for model in models:\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, model_predictions[model])\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
