{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Sentiment Analysis\n",
    "\n",
    "Project by Abhi Vellore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_process import AmazonDatasetPreprocessor, KaggleDatasetPreprocessor, SentenceDatasetPreprocessor\n",
    "from config import PROCESSED_DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "from gpt import ChatGPTSession\n",
    "import glob\n",
    "from llama import LLaMaSession\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from credentials import openai_api_key, llama_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/processed/processed_amazon_data.csv\n",
      "Processed data saved to ../data/processed/processed_sentence_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed combined Kaggle data saved to ../data/processed/processed_kaggle_combined_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "amazon_processor = AmazonDatasetPreprocessor('Amazon_Fashion_Review_Data.json')\n",
    "amazon_processor.preprocess()\n",
    "amazon_processor.to_csv('processed_amazon_data.csv')\n",
    "\n",
    "# Initialize and process the Sentence data\n",
    "sentence_processor = SentenceDatasetPreprocessor('Sentences_75Agree.txt')\n",
    "sentence_processor.preprocess()\n",
    "sentence_processor.to_csv('processed_sentence_data.csv')\n",
    "\n",
    "# Initialize and process the first Kaggle dataset\n",
    "kaggle1_processor = KaggleDatasetPreprocessor('kaggle_train.csv')\n",
    "kaggle1_processor.preprocess()\n",
    "\n",
    "# Initialize and process the second Kaggle dataset\n",
    "kaggle2_processor = KaggleDatasetPreprocessor('kaggle_test.csv')\n",
    "kaggle2_processor.preprocess()\n",
    "\n",
    "# Concatenate the preprocessed DataFrames\n",
    "processed_kaggle1_df = kaggle1_processor.df\n",
    "processed_kaggle2_df = kaggle2_processor.df\n",
    "combined_df = pd.concat([processed_kaggle1_df, processed_kaggle2_df], ignore_index=True)\n",
    "\n",
    "# Sample approximately 3000 rows from the combined DataFrame\n",
    "sampled_df = combined_df.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file in the processed directory\n",
    "combined_csv_path = os.path.join(PROCESSED_DATA_PATH, 'processed_kaggle_combined_data.csv')\n",
    "sampled_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"Processed combined Kaggle data saved to {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and Save Functions\n",
    "\n",
    "Create a series of functions with checks to prevent hitting OpenAI rate limit safeguards and track expenses of running the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define state-saving functions\n",
    "def save_state(state_file, last_processed_index):\n",
    "    with open(state_file, 'w') as file:\n",
    "        file.write(str(last_processed_index))\n",
    "\n",
    "def load_state(state_file):\n",
    "    try:\n",
    "        with open(state_file, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_limit = 10000\n",
    "rate_limit_per_minute = 500\n",
    "state_file_path = 'last_processed_line.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model\n",
    "\n",
    "Create GPT models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with GPT - saves into new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_GPTModel(input_csv_path, output_csv_path, chat_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = chat_session.send_prompt(row['Text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GPT Models\n",
    "\n",
    "Sets context to minimize tokens being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_context = \"As a sentiment analysis model, rate the sentiment of the following text from 1 to 5, where 1 is very negative and 5 is very positive. Provide only the number as a response.\"\n",
    "\n",
    "# Initialize sessions for GPT-3.5-Turbo and GPT-4.0\n",
    "session_gpt_3_5 = ChatGPTSession(api_key=openai_api_key, model='gpt-3.5-turbo', rate_limit_per_minute=1000)\n",
    "session_gpt_4 = ChatGPTSession(api_key=openai_api_key, model='gpt-4', rate_limit_per_minute=300)\n",
    "\n",
    "session_gpt_3_5.set_context(sentiment_context)\n",
    "session_gpt_4.set_context(sentiment_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Financial Data\n",
    "\n",
    "Split into two separate commands to ensure accuracy. Will use a for loop in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data_.csv' with GPT-3.5\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data.csv'\n",
    "output_sentence_path_3_5 = sentence_data_filename.replace('.csv', '_with_gpt_3.5.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_3_5, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_3_5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'sentence_data_with_gpt_3_5.csv' with GPT-4.0\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_3_5.csv'\n",
    "output_sentence_path_4 = sentence_data_filename.replace('_with_gpt_3_5.csv', '_with_gpt_scores.csv')\n",
    "df_sentence = process_dataset_with_GPTModel(sentence_data_filename, output_sentence_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with GPT-4.0 and saved to {output_sentence_path_4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing other datasets\n",
    "\n",
    "After confirming setup works for the financial dataset, process the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process other datasets with GPT-3.5 and GPT-4.0\n",
    "other_datasets = ['../data/processed/processed_amazon_data.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data.csv']\n",
    "\n",
    "for dataset in other_datasets:\n",
    "    save_state(state_file_path, -1)\n",
    "    # Process with GPT-3.5\n",
    "    output_path_3_5 = dataset.replace('.csv', '_with_gpt_3_5.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_3_5, session_gpt_3_5, \"GPT 3.5 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-3.5 and saved to {output_path_3_5}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-3.5 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run GPT-4.0 processing tomorrow.\")\n",
    "        continue  # Continue to the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in other_datasets:\n",
    "    # Process with GPT-4.0\n",
    "    output_path_4 = dataset.replace('.csv', '_with_gpt_scores.csv')\n",
    "    df = process_dataset_with_GPTModel(dataset, output_path_4, session_gpt_4, \"GPT 4.0 Score\", state_file_path)\n",
    "    print(f\"Dataset processed with GPT-4.0 and saved to {output_path_4}\")\n",
    "    \n",
    "    # Check if daily limit reached after GPT-4.0 processing\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        continue  # Continue to the next dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama Models\n",
    "Create Llama models and use them to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process a dataset with Llama-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_llama_model(input_csv_path, output_csv_path, llama_session, column_name, state_file):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    start_index = load_state(state_file)  # Load the last processed index\n",
    "    processed_count = 0\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < start_index:\n",
    "            continue  # Skip already processed rows\n",
    "\n",
    "        # Check if we've reached the daily limit before processing the next row\n",
    "        if processed_count >= daily_limit:\n",
    "            print(\"Reached the daily limit, stopping...\")\n",
    "            break\n",
    "\n",
    "        # Insert your API call here and store the response\n",
    "        response = llama_session.send_prompt(row['text'])\n",
    "        df.at[index, column_name] = response\n",
    "\n",
    "        # Save the state after each line is processed\n",
    "        save_state(state_file, index)\n",
    "        processed_count += 1\n",
    "\n",
    "        # Handle rate limiting\n",
    "        if processed_count % rate_limit_per_minute == 0 and processed_count != 0:\n",
    "            print(\"Rate limit reached, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Save the modified DataFrame\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Processing completed. Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sessions for Llama-7b\n",
    "session_llama_7b = LLaMaSession(api_key=llama_api_key, model='llama-7b-chat', rate_limit_per_minute=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Financial Data\n",
    "\n",
    "Split into two separate commands to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'processed_sentence_data__with_gpt_scores.csv' with GPT-\n",
    "sentence_data_filename = '../data/processed/processed_sentence_data_with_gpt_scores.csv'\n",
    "output_sentence_path_7b = sentence_data_filename.replace('_with_gpt_scores.csv', '_all_scores.csv')\n",
    "process_dataset_with_llama_model(dataset, output_sentence_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "print(f\"Processed {sentence_data_filename} with llama and saved to {output_sentence_path_7b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing other datasets\n",
    "\n",
    "After confirming setup works for the financial dataset, process the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_datasets = ['../data/processed/processed_amazon_data_with_gpt_scores.csv', \n",
    "                  '../data/processed/processed_kaggle_combined_data_with_gpt_scores.csv']\n",
    "\n",
    "# Process each dataset with both models\n",
    "for dataset in other_datasets:\n",
    "    output_path_7b = dataset.replace('.csv', '_all_scores.csv')\n",
    "    process_dataset_with_llama_model(dataset, output_path_7b, session_llama_7b, \"Llama 7B Score\", state_file=state_file_path)\n",
    "    print(f\"Dataset processed with Llama-7B and saved to {output_path_7b}\")\n",
    "\n",
    "\n",
    "    current_index = load_state(state_file_path)\n",
    "    if current_index + 1 >= daily_limit:\n",
    "        print(\"Daily limit reached, please run again tomorrow.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, model_name, dataset_name):\n",
    "    # Numeric evaluation\n",
    "    accuracy_numeric = accuracy_score(actuals, predictions)\n",
    "    f1score_numeric = f1_score(actuals, predictions, average='weighted')\n",
    "    report_numeric = classification_report(actuals, predictions)\n",
    "    \n",
    "    # Categorical mapping and evaluation\n",
    "    label_mapping = {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}\n",
    "    categorical_actuals = actuals.map(label_mapping)\n",
    "    categorical_predictions = predictions.map(label_mapping)\n",
    "    accuracy_categorical = accuracy_score(categorical_actuals, categorical_predictions)\n",
    "    f1score_categorical = f1_score(categorical_actuals, categorical_predictions, average='weighted', labels=['negative', 'neutral', 'positive'])\n",
    "    report_categorical = classification_report(categorical_actuals, categorical_predictions, labels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    # Store results in dictionaries\n",
    "    numeric_results = {\n",
    "        'Accuracy (Numeric)': accuracy_numeric,\n",
    "        'F1 Score (Numeric)': f1score_numeric,\n",
    "        'Classification Report (Numeric)': report_numeric\n",
    "    }\n",
    "    \n",
    "    categorical_results = {\n",
    "        'Accuracy (Categorical)': accuracy_categorical,\n",
    "        'F1 Score (Categorical)': f1score_categorical,\n",
    "        'Classification Report (Categorical)': report_categorical\n",
    "    }\n",
    "    \n",
    "    # Optionally, print the results\n",
    "    print(f\"--- Numeric Evaluation for {model_name} on {dataset_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_numeric:.2f}\")\n",
    "    print(f\"F1 Score: {f1score_numeric:.2f}\")\n",
    "    print(report_numeric)\n",
    "    \n",
    "    print(f\"--- Categorical Evaluation for {model_name} on {dataset_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_categorical:.2f}\")\n",
    "    print(f\"F1 Score: {f1score_categorical:.2f}\")\n",
    "    print(report_categorical)\n",
    "    \n",
    "    return numeric_results, categorical_results\n",
    "\n",
    "def plot_confusion_matrix(predictions, actuals, model_name, dataset_name):\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Define the labels for your classes\n",
    "    labels = [1, 2, 3, 4, 5]\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name} - {dataset_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics per Model per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['processed_sentence_data_all_scores.csv', 'processed_kaggle_combined_all_scores.csv', 'processed_amazon_data_all_scores.csv'] \n",
    "models = ['GPT_3_5', 'GPT_4', 'LLaMA_7B']  # Replace with your actual model names\n",
    "metrics_df = pd.DataFrame()\n",
    "all_scores_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dataset_file in datasets:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    actuals = df['Actual_Score']\n",
    "    for model in models:\n",
    "        predictions = df[model]\n",
    "        numeric_results, categorical_results = evaluate_model(predictions, actuals, model, dataset_file)\n",
    "        plot_confusion_matrix(predictions, actuals, model, dataset_file)\n",
    "\n",
    "        # Flatten the results into a single dictionary for the DataFrame\n",
    "        results_to_store = {\n",
    "            'Dataset': dataset_file,\n",
    "            'Model': model,\n",
    "            'Accuracy (Numeric)': numeric_results['Accuracy (Numeric)'],\n",
    "            'F1 Score (Numeric)': numeric_results['F1 Score (Numeric)'],\n",
    "            'Accuracy (Categorical)': categorical_results['Accuracy (Categorical)'],\n",
    "            'F1 Score (Categorical)': categorical_results['F1 Score (Categorical)']\n",
    "        }\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([results_to_store])], ignore_index=True)\n",
    "\n",
    "        if all_scores_df.empty:\n",
    "            # If all_scores_df is empty, initialize it with the score columns\n",
    "            all_scores_df = df[models].copy()\n",
    "        else:\n",
    "            # If all_scores_df already contains data, concatenate along the columns (axis=1)\n",
    "            all_scores_df = pd.concat([all_scores_df, df[models]], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated Metrics Across all Datasets Per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results by taking the mean of each metric per model\n",
    "aggregated_metrics = metrics_df.groupby('Model').agg({\n",
    "    'Accuracy (Numeric)': 'mean',\n",
    "    'F1 Score (Numeric)': 'mean',\n",
    "    'Accuracy (Categorical)': 'mean',\n",
    "    'F1 Score (Categorical)': 'mean'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.35\n",
    "index = pd.Index(range(len(aggregated_metrics)))\n",
    "\n",
    "# Create combined plots for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index - bar_width/2, aggregated_metrics['Accuracy (Numeric)'], bar_width, label='Accuracy (Numeric)', color='lightgreen')\n",
    "plt.bar(index + bar_width/2, aggregated_metrics['Accuracy (Categorical)'], bar_width, label='Accuracy (Categorical)', color='lightblue')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(index, aggregated_metrics['Model'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparative Accuracy Across Models')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create combined plots for F1 score\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index - bar_width/2, aggregated_metrics['F1 Score (Numeric)'], bar_width, label='F1 Score (Numeric)', color='lightcoral')\n",
    "plt.bar(index + bar_width/2, aggregated_metrics['F1 Score (Categorical)'], bar_width, label='F1 Score (Categorical)', color='lightsalmon')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(index, aggregated_metrics['Model'])\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Comparative F1 Score Across Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions and actuals for each model\n",
    "model_predictions = {model: [] for model in models}\n",
    "actual_labels = []\n",
    "\n",
    "# Loop over the dataset files\n",
    "for dataset_file in datasets:\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    # Extend the actual labels list\n",
    "    actual_labels.extend(df['Actual_Score'])\n",
    "    \n",
    "    # For each model, extend the corresponding predictions list\n",
    "    for model in models:\n",
    "        model_predictions[model].extend(df[model])\n",
    "\n",
    "# Now create and plot a confusion matrix for each model\n",
    "for model in models:\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, model_predictions[model])\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
